<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script></script><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3"><link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222"><meta name="keywords" content="Python,爬虫,搜索引擎,Scrapy,"><link rel="alternate" href="/atom.xml" title="mtianyan's blog" type="application/atom+xml"><meta name="description" content="五、爬虫与反爬虫  介绍反爬虫的基本知识，随机更换useagent，fake UseAgent代理池，西刺代理创建ip代理池，云打码实现验证码的识别等。"><meta name="keywords" content="Python,爬虫,搜索引擎,Scrapy"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争"><meta property="og:url" content="http://blog.mtianyan.cn/post/791a397f.html"><meta property="og:site_name" content="mtianyan&#39;s blog"><meta property="og:description" content="五、爬虫与反爬虫  介绍反爬虫的基本知识，随机更换useagent，fake UseAgent代理池，西刺代理创建ip代理池，云打码实现验证码的识别等。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-d48e923ecb3d20ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-77314219c9f1757e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-8381473a93549bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1779926-8f93d40d66c9fe04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><meta property="og:updated_time" content="2018-01-28T21:53:17.952Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争"><meta name="twitter:description" content="五、爬虫与反爬虫  介绍反爬虫的基本知识，随机更换useagent，fake UseAgent代理池，西刺代理创建ip代理池，云打码实现验证码的识别等。"><meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1779926-d48e923ecb3d20ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.3",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.mtianyan.cn/post/791a397f.html"><script>!function(e,t,o,c,i,a,n){e.DaoVoiceObject=i,e[i]=e[i]||function(){(e[i].q=e[i].q||[]).push(arguments)},e[i].l=1*new Date,a=t.createElement("script"),n=t.getElementsByTagName("script")[0],a.async=1,a.src=c,a.charset="utf-8",n.parentNode.insertBefore(a,n)}(window,document,0,("https:"==document.location.protocol?"https:":"http:")+"//widget.daovoice.io/widget/0f81ff2f.js","daovoice"),daovoice("init",{app_id:"e28768be"}),daovoice("update")</script><title>Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争 | mtianyan's blog</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?415372bd35fec36f7558dd96b48ec03f";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div> <a href="https://github.com/mtianyan" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">mtianyan's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">天涯明月笙的博客小站(Github托管)</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br> 公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.mtianyan.cn/post/791a397f.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="mtianyan"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="mtianyan's blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-02T13:36:48+08:00">2017-07-02</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Scrapy分布式爬虫打造搜索引擎/" itemprop="url" rel="index"><span itemprop="name">Scrapy分布式爬虫打造搜索引擎</span></a></span></span> <span id="/post/791a397f.html" class="leancloud_visitors" data-flag-title="Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">热度&#58;</span><span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">2,628</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">14</span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><p>五、爬虫与反爬虫</p></blockquote><div class="note warning"><p>介绍反爬虫的基本知识，随机更换useagent，fake UseAgent代理池，西刺代理创建ip代理池，云打码实现验证码的识别等。</p></div><a id="more"></a><h3 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a>1. 基础知识</h3><p>如何使我们的爬虫不被禁止掉</p><p>爬虫：</p><blockquote><p>自动获取数据的程序，关键是批量的获取</p></blockquote><p>反爬虫：</p><blockquote><p>使用技术手段防止爬虫程序的方法</p></blockquote><p>误伤：</p><blockquote><p>反爬虫技术将普通用户识别为爬虫，效果再好也不能用</p></blockquote><p>学校，网吧，出口的公网ip只有一个，所以禁止ip不能用。</p><p>ip动态分配。a爬封b</p><p>成本：</p><blockquote><p>反爬虫人力和机器成本</p></blockquote><p>拦截：</p><blockquote><p>拦截率越高，误伤率越高</p></blockquote><p>反爬虫的目的：</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-d48e923ecb3d20ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="反爬虫的目的"></p><p>爬虫与反爬虫的对抗过程：</p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-77314219c9f1757e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="爬虫与反爬虫斗争"></p><p>使用检查可以查看到价格，而查看网页源代码无法查看到价格字段。<br>scrapy下载到的网页时网页源代码。<br>js（ajax）填充的动态数据无法通过网页获取到。</p><h3 id="2-scrapy架构及源码介绍"><a href="#2-scrapy架构及源码介绍" class="headerlink" title="2. scrapy架构及源码介绍"></a>2. scrapy架构及源码介绍</h3><p><img src="http://upload-images.jianshu.io/upload_images/1779926-8381473a93549bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="scrapy组件分析图"></p><p><img src="http://upload-images.jianshu.io/upload_images/1779926-8f93d40d66c9fe04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="scrapy官方架构图"></p><ol><li>我们编写的spider，然后yield一个request发送给engine</li><li>engine拿到什么都不做然后给scheduler</li><li>engine会生成一个request给engine</li><li>engine拿到之后通过downloadermiddleware 给downloader</li><li>downloader再发送response回来给engine。</li><li>engine拿到之后，response给spider。</li><li>spider进行处理，解析出item &amp; request，</li><li>item-&gt;给itempipeline；如果是request，跳转步骤二</li></ol><p>path：articlespider3\Lib\site-packages\scrapy\core</p><ul><li>engine.py：</li><li>scheduler.py</li><li><p>downloader</p></li><li><p>item</p></li><li>pipeline</li><li>spider</li></ul><p><strong>engine.py：重要函数schedule</strong></p><ol><li>enqueue_request：把request放scheduler</li><li><code>_next_request_from_scheduler</code>:从调度器拿。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">schedule</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">    self.signals.send_catch_log(signal=signals.request_scheduled,</span><br><span class="line">            request=request, spider=spider)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.slot.scheduler.enqueue_request(request):</span><br><span class="line">        self.signals.send_catch_log(signal=signals.request_dropped,</span><br><span class="line">                                    request=request, spider=spider)</span><br></pre></td></tr></table></figure><p>articlespider3\Lib\site-packages\scrapy\core\downloader\handlers</p><blockquote><p>支持文件，ftp，http下载(https).</p></blockquote><p>后期定制middleware：</p><ul><li>spidermiddlewire</li><li>downloadmiddlewire</li></ul><p>django和scrapy结构类似</p><h3 id="3-scrapy的两个重要类：request和response"><a href="#3-scrapy的两个重要类：request和response" class="headerlink" title="3. scrapy的两个重要类：request和response"></a>3. scrapy的两个重要类：request和response</h3><p>类似于django httprequest</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url))</span><br></pre></td></tr></table></figure><p>request参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Request</span><span class="params">(object_ref)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, callback=None, method=<span class="string">'GET'</span>, headers=None, body=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 cookies=None, meta=None, encoding=<span class="string">'utf-8'</span>, priority=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dont_filter=False, errback=None)</span>:</span></span><br></pre></td></tr></table></figure><p>cookies：<br>Lib\site-packages\scrapy\downloadermiddlewares\cookies.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookiejarkey = request.meta.get(<span class="string">"cookiejar"</span>)</span><br></pre></td></tr></table></figure><ul><li>priority: 优先级，影响调度顺序</li><li>dont_filter：我的同样的request不会被过滤</li><li>errback：错误时的回调函数</li></ul><p><a href="https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response" target="_blank" rel="noopener">https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response</a></p><p><strong>errback example：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ErrbackSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"errback_example"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.httpbin.org/"</span>,              <span class="comment"># HTTP 200 expected</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org/status/404"</span>,    <span class="comment"># Not found error</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org/status/500"</span>,    <span class="comment"># server issue</span></span><br><span class="line">        <span class="string">"http://www.httpbin.org:12345/"</span>,        <span class="comment"># non-responding host, timeout expected</span></span><br><span class="line">        <span class="string">"http://www.httphttpbinbin.org/"</span>,       <span class="comment"># DNS error expected</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(u, callback=self.parse_httpbin,</span><br><span class="line">                                    errback=self.errback_httpbin,</span><br><span class="line">                                    dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_httpbin</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Got successful response from &#123;&#125;'</span>.format(response.url))</span><br><span class="line">        <span class="comment"># do something useful here...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errback_httpbin</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">        <span class="comment"># log all failures</span></span><br><span class="line">        self.logger.error(repr(failure))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># in case you want to do something special for some errors,</span></span><br><span class="line">        <span class="comment"># you may need the failure's type:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> failure.check(HttpError):</span><br><span class="line">            <span class="comment"># these exceptions come from HttpError spider middleware</span></span><br><span class="line">            <span class="comment"># you can get the non-200 response</span></span><br><span class="line">            response = failure.value.response</span><br><span class="line">            self.logger.error(<span class="string">'HttpError on %s'</span>, response.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> failure.check(DNSLookupError):</span><br><span class="line">            <span class="comment"># this is the original request</span></span><br><span class="line">            request = failure.request</span><br><span class="line">            self.logger.error(<span class="string">'DNSLookupError on %s'</span>, request.url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> failure.check(TimeoutError, TCPTimedOutError):</span><br><span class="line">            request = failure.request</span><br><span class="line">            self.logger.error(<span class="string">'TimeoutError on %s'</span>, request.url)</span><br></pre></td></tr></table></figure><p><strong>response类</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, status=<span class="number">200</span>, headers=None, body=<span class="string">b''</span>, flags=None, request=None)</span>:</span></span><br><span class="line">       self.headers = Headers(headers <span class="keyword">or</span> &#123;&#125;)</span><br></pre></td></tr></table></figure><p><strong>response的参数：</strong><br>request：yield出来的request，会放在response，让我们知道它是从哪里来的</p><h3 id="4-自行编写随机更换useagent"><a href="#4-自行编写随机更换useagent" class="headerlink" title="4. 自行编写随机更换useagent"></a>4. 自行编写随机更换useagent</h3><ol><li>setting中设置</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user_agent_list = [</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'</span>,</span><br><span class="line">    <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36'</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>然后在代码中使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> settings <span class="keyword">import</span> user_agent_list</span><br><span class="line">   <span class="keyword">import</span> random</span><br><span class="line">   random_index =random.randint(<span class="number">0</span>,len(user_agent_list))</span><br><span class="line">   random_agent = user_agent_list[random_index]</span><br><span class="line"></span><br><span class="line">   <span class="string">'User-Agent'</span>: random_agent</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random_index = random.randint(<span class="number">0</span>, len(user_agent_list))</span><br><span class="line">random_agent = user_agent_list[random_index]</span><br><span class="line">self.headers[<span class="string">"User-Agent"</span>] = random_agent</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(request_url, headers=self.headers, callback=self.parse_question)</span><br></pre></td></tr></table></figure><p>但是问题：每个request之前都得这样做。</p><h3 id="5-middlewire配置及编写fake-UseAgent代理池"><a href="#5-middlewire配置及编写fake-UseAgent代理池" class="headerlink" title="5. middlewire配置及编写fake UseAgent代理池"></a>5. middlewire配置及编写fake UseAgent代理池</h3><p>取消DOWNLOADER_MIDDLEWARES的注释状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.middlewares.MyCustomDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>articlespider3\Lib\site-packages\scrapy\downloadermiddlewares\useragent.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""This middleware allows spiders to override the user_agent"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, user_agent=<span class="string">'Scrapy'</span>)</span>:</span></span><br><span class="line">        self.user_agent = user_agent</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        o = cls(crawler.settings[<span class="string">'USER_AGENT'</span>])</span><br><span class="line">        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> o</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.user_agent = getattr(spider, <span class="string">'user_agent'</span>, self.user_agent)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.user_agent:</span><br><span class="line">            request.headers.setdefault(<span class="string">b'User-Agent'</span>, self.user_agent)</span><br></pre></td></tr></table></figure><p>重要方法process_request</p><p><strong>配置默认useagent为none</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'ArticleSpider.middlewares.MyCustomDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用fakeuseragent</strong><br><code>pip install fake-useragent</code></p><p>setting.py设置随机模式<code>RANDOM_UA_TYPE = &quot;random&quot;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddlware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#随机更换user-agent</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">        super(RandomUserAgentMiddlware, self).__init__()</span><br><span class="line">        self.ua = UserAgent()</span><br><span class="line">        self.ua_type = crawler.settings.get(<span class="string">"RANDOM_UA_TYPE"</span>, <span class="string">"random"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(crawler)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_ua</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">return</span> getattr(self.ua, self.ua_type)</span><br><span class="line"></span><br><span class="line">        request.headers.setdefault(<span class="string">'User-Agent'</span>, get_ua())</span><br></pre></td></tr></table></figure><h3 id="6-使用西刺代理创建ip代理池保存到数据库"><a href="#6-使用西刺代理创建ip代理池保存到数据库" class="headerlink" title="6. 使用西刺代理创建ip代理池保存到数据库*"></a>6. 使用西刺代理创建ip代理池保存到数据库*</h3><p>ip动态变化：重启路由器等</p><p>ip代理的原理：</p><p>不直接发送自己真实ip，而使用中间代理商（代理服务器），那么服务器不知道我们的ip也就不会把我们禁掉<br>setting.py设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#动态设置ip代理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = <span class="string">"http://111.198.219.151:8118"</span></span><br></pre></td></tr></table></figure><p><strong>使用西刺代理创建代理池保存到数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/5/24 16:27'</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line">conn = MySQLdb.connect(host=<span class="string">"127.0.0.1"</span>, user=<span class="string">"root"</span>, passwd=<span class="string">"ty158917"</span>, db=<span class="string">"article_spider"</span>, charset=<span class="string">"utf8"</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl_ips</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#爬取西刺的免费ip代理</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0"</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1568</span>):</span><br><span class="line">        re = requests.get(<span class="string">"http://www.xicidaili.com/nn/&#123;0&#125;"</span>.format(i), headers=headers)</span><br><span class="line"></span><br><span class="line">        selector = Selector(text=re.text)</span><br><span class="line">        all_trs = selector.css(<span class="string">"#ip_list tr"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ip_list = []</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> all_trs[<span class="number">1</span>:]:</span><br><span class="line">            speed_str = tr.css(<span class="string">".bar::attr(title)"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> speed_str:</span><br><span class="line">                speed = float(speed_str.split(<span class="string">"秒"</span>)[<span class="number">0</span>])</span><br><span class="line">            all_texts = tr.css(<span class="string">"td::text"</span>).extract()</span><br><span class="line"></span><br><span class="line">            ip = all_texts[<span class="number">0</span>]</span><br><span class="line">            port = all_texts[<span class="number">1</span>]</span><br><span class="line">            proxy_type = all_texts[<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">            ip_list.append((ip, port, proxy_type, speed))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> ip_list:</span><br><span class="line">            cursor.execute(</span><br><span class="line">                <span class="string">"insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, 'HTTP')"</span>.format(</span><br><span class="line">                    ip_info[<span class="number">0</span>], ip_info[<span class="number">1</span>], ip_info[<span class="number">3</span>]</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            conn.commit()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetIP</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete_ip</span><span class="params">(self, ip)</span>:</span></span><br><span class="line">        <span class="comment">#从数据库中删除无效的ip</span></span><br><span class="line">        delete_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            delete from proxy_ip where ip='&#123;0&#125;'</span></span><br><span class="line"><span class="string">        """</span>.format(ip)</span><br><span class="line">        cursor.execute(delete_sql)</span><br><span class="line">        conn.commit()</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge_ip</span><span class="params">(self, ip, port)</span>:</span></span><br><span class="line">        <span class="comment">#判断ip是否可用</span></span><br><span class="line">        http_url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">        proxy_url = <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            proxy_dict = &#123;</span><br><span class="line">                <span class="string">"http"</span>:proxy_url,</span><br><span class="line">            &#125;</span><br><span class="line">            response = requests.get(http_url, proxies=proxy_dict)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"invalid ip and port"</span>)</span><br><span class="line">            self.delete_ip(ip)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code = response.status_code</span><br><span class="line">            <span class="keyword">if</span> code &gt;= <span class="number">200</span> <span class="keyword">and</span> code &lt; <span class="number">300</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"effective ip"</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span>  (<span class="string">"invalid ip and port"</span>)</span><br><span class="line">                self.delete_ip(ip)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_ip</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#从数据库中随机获取一个可用的ip</span></span><br><span class="line">        random_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">              SELECT ip, port FROM proxy_ip</span></span><br><span class="line"><span class="string">            ORDER BY RAND()</span></span><br><span class="line"><span class="string">            LIMIT 1</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">        result = cursor.execute(random_sql)</span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line">            ip = ip_info[<span class="number">0</span>]</span><br><span class="line">            port = ip_info[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            judge_re = self.judge_ip(ip, port)</span><br><span class="line">            <span class="keyword">if</span> judge_re:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> self.get_random_ip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print (crawl_ips())</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    get_ip = GetIP()</span><br><span class="line">    get_ip.get_random_ip()</span><br></pre></td></tr></table></figure><p><strong>使用scrapy_proxies创建ip代理池</strong></p><p><code>pip install scrapy_proxies</code></p><p>收费，但是简单<br><a href="https://github.com/scrapy-plugins/scrapy-crawlera" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-crawlera</a></p><p>tor隐藏。vpn<br><a href="http://www.theonionrouter.com/" target="_blank" rel="noopener">http://www.theonionrouter.com/</a></p><h3 id="7-通过云打码实现验证码的识别"><a href="#7-通过云打码实现验证码的识别" class="headerlink" title="7. 通过云打码实现验证码的识别"></a>7. 通过云打码实现验证码的识别</h3><p><a href="http://www.yundama.com/" target="_blank" rel="noopener">http://www.yundama.com/</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/6/24 16:48'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YDMHttp</span><span class="params">(object)</span>:</span></span><br><span class="line">    apiurl = <span class="string">'http://api.yundama.com/api.php'</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    appid = <span class="string">''</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, appid, appkey)</span>:</span></span><br><span class="line">        self.username = username</span><br><span class="line">        self.password = password</span><br><span class="line">        self.appid = str(appid)</span><br><span class="line">        self.appkey = appkey</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">balance</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'balance'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"获取剩余积分"</span>, ret_data[<span class="string">"balance"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"balance"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'login'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"登录成功"</span>, ret_data[<span class="string">"uid"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"uid"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, filename, codetype, timeout)</span>:</span></span><br><span class="line">        data = &#123;<span class="string">'method'</span>: <span class="string">'upload'</span>, <span class="string">'username'</span>: self.username, <span class="string">'password'</span>: self.password, <span class="string">'appid'</span>: self.appid, <span class="string">'appkey'</span>: self.appkey, <span class="string">'codetype'</span>: str(codetype), <span class="string">'timeout'</span>: str(timeout)&#125;</span><br><span class="line">        files = &#123;<span class="string">'file'</span>: open(filename, <span class="string">'rb'</span>)&#125;</span><br><span class="line">        response_data = requests.post(self.apiurl, files=files, data=data)</span><br><span class="line">        ret_data = json.loads(response_data.text)</span><br><span class="line">        <span class="keyword">if</span> ret_data[<span class="string">"ret"</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"识别成功"</span>, ret_data[<span class="string">"text"</span>])</span><br><span class="line">            <span class="keyword">return</span> ret_data[<span class="string">"text"</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ydm</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 密码</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appid = </span><br><span class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 图片文件</span></span><br><span class="line">    filename = <span class="string">'image/1.jpg'</span></span><br><span class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></span><br><span class="line">    codetype = <span class="number">5000</span></span><br><span class="line">    <span class="comment"># 超时时间，秒</span></span><br><span class="line">    timeout = <span class="number">60</span></span><br><span class="line">    <span class="comment"># 检查</span></span><br><span class="line"></span><br><span class="line">    yundama = YDMHttp(username, password, appid, appkey)</span><br><span class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</span><br><span class="line">        print(<span class="string">'请设置好相关参数再测试'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></span><br><span class="line">        <span class="keyword">return</span> yundama.decode(file_path, codetype, timeout);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 用户名</span></span><br><span class="line">    username = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 密码</span></span><br><span class="line">    password = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appid = </span><br><span class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appkey = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 图片文件</span></span><br><span class="line">    filename = <span class="string">'image/captcha.jpg'</span></span><br><span class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></span><br><span class="line">    codetype = <span class="number">5000</span></span><br><span class="line">    <span class="comment"># 超时时间，秒</span></span><br><span class="line">    timeout = <span class="number">60</span></span><br><span class="line">    <span class="comment"># 检查</span></span><br><span class="line">    <span class="keyword">if</span> (username == <span class="string">'username'</span>):</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'请设置好相关参数再测试'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        yundama = YDMHttp(username, password, appid, appkey)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 登陆云打码</span></span><br><span class="line">        uid = yundama.login();</span><br><span class="line">        print(<span class="string">'uid: %s'</span> % uid)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 登陆云打码</span></span><br><span class="line">        uid = yundama.login();</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'uid: %s'</span> % uid)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查询余额</span></span><br><span class="line">        balance = yundama.balance();</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'balance: %s'</span> % balance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></span><br><span class="line">        text = yundama.decode(filename, codetype, timeout);</span><br></pre></td></tr></table></figure><h3 id="8-cookie的禁用。-amp-设置下载速度"><a href="#8-cookie的禁用。-amp-设置下载速度" class="headerlink" title="8. cookie的禁用。&amp; 设置下载速度"></a>8. cookie的禁用。&amp; 设置下载速度</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html</a></p><p>setting.py:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line">COOKIES_ENABLED = <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>设置下载速度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5</span></span><br></pre></td></tr></table></figure><p>给不同的spider设置自己的setting值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    <span class="string">"COOKIES_ENABLED"</span>: <span class="keyword">True</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/post/791a397f.html">Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争</a></p><p><span>文章作者:</span><a href="/" title="访问 mtianyan 的个人博客">mtianyan</a></p><p><span>发布时间:</span>2017年07月02日 - 13:07</p><p><span>最后更新:</span>2018年01月29日 - 05:01</p><p><span>原始链接:</span><a href="/post/791a397f.html" title="Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争">http://blog.mtianyan.cn/post/791a397f.html</a><span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://blog.mtianyan.cn/post/791a397f.html" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"",text:"复制成功",icon:"success",showConfirmButton:!0})})})</script></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>请博主吃包辣条</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="/images/wechatpay.png" alt="mtianyan 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"> <img id="alipay_qr" src="/images/alipay.jpg" alt="mtianyan 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a><a href="/tags/爬虫/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a><a href="/tags/搜索引擎/" rel="tag"><i class="fa fa-tag"></i> 搜索引擎</a><a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/d083b798.html" rel="next" title="Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取"><i class="fa fa-chevron-left"></i> Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/post/fb760a12.html" rel="prev" title="Scrapy分布式爬虫打造搜索引擎- (六)scrapy进阶开发">Scrapy分布式爬虫打造搜索引擎- (六)scrapy进阶开发<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"><div class="jiathis_style"> <span class="jiathis_txt">分享到：</span> <a class="jiathis_button_fav">收藏夹</a> <a class="jiathis_button_copy">复制网址</a> <a class="jiathis_button_email">邮件</a> <a class="jiathis_button_weixin">微信</a> <a class="jiathis_button_qzone">QQ空间</a> <a class="jiathis_button_tqq">腾讯微博</a> <a class="jiathis_button_douban">豆瓣</a> <a class="jiathis_button_share">一键分享</a> <a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a><a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={data_track_clickback:!0,summary:"",shortUrl:!1,hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=2154292" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="SOHUCS"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="mtianyan"><p class="site-author-name" itemprop="name">mtianyan</p><p class="site-description motion-element" itemprop="description">爱分享，爱技术，爱生活。</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">22</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/mtianyan" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.jianshu.com/u/db9a7a0daa1f" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书</a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/114963812195952881148" target="_blank" title="Google"><i class="fa fa-fw fa-google"></i> Google</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="http://mtianyan.gitee.io/" title="本站孪生站(国内码云托管)" target="_blank">本站孪生站(国内码云托管)</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-基础知识"><span class="nav-number">1.</span> <span class="nav-text">1. 基础知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-scrapy架构及源码介绍"><span class="nav-number">2.</span> <span class="nav-text">2. scrapy架构及源码介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-scrapy的两个重要类：request和response"><span class="nav-number">3.</span> <span class="nav-text">3. scrapy的两个重要类：request和response</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-自行编写随机更换useagent"><span class="nav-number">4.</span> <span class="nav-text">4. 自行编写随机更换useagent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-middlewire配置及编写fake-UseAgent代理池"><span class="nav-number">5.</span> <span class="nav-text">5. middlewire配置及编写fake UseAgent代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-使用西刺代理创建ip代理池保存到数据库"><span class="nav-number">6.</span> <span class="nav-text">6. 使用西刺代理创建ip代理池保存到数据库*</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-通过云打码实现验证码的识别"><span class="nav-number">7.</span> <span class="nav-text">7. 通过云打码实现验证码的识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-cookie的禁用。-amp-设置下载速度"><span class="nav-number">8.</span> <span class="nav-text">8. cookie的禁用。&amp; 设置下载速度</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2018</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">mtianyan</span><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共178.0k字</span></div></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span class="site-uv">本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次</span> <span class="site-pv">本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><script type="text/javascript">!function(){var t="1eb79150519fd9b791c77e8eef6f3632";if((window.innerWidth||document.documentElement.clientWidth)<960)window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=cyrJmJ3rL&conf='+t+'"><\/script>');else{!function(t,e){var n=document.getElementsByTagName("head")[0]||document.head||document.documentElement,a=document.createElement("script");a.setAttribute("type","text/javascript"),a.setAttribute("charset","UTF-8"),a.setAttribute("src",t),"function"==typeof e&&(window.attachEvent?a.onreadystatechange=function(){var t=a.readyState;"loaded"!==t&&"complete"!==t||(a.onreadystatechange=null,e())}:a.onload=e),n.appendChild(a)}("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:"cyrJmJ3rL",conf:t})})}}()</script><script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script><script type="text/javascript">var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")};function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){var r=!1,s=0,a=0,i=n.title.trim(),c=i.toLowerCase(),l=n.content.trim().replace(/<[^>]+>/g,""),h=l.toLowerCase(),p=decodeURIComponent(n.url),u=[],f=[];if(""!=i&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}u=u.concat(e(t,c,!1)),f=f.concat(e(t,h,!1))}),(u.length>0||f.length>0)&&(r=!0,s=u.length+f.length)),r){[u,f].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});function d(e,o,n,r){for(var s=r[r.length-1],i=s.position,c=s.word,l=[],h=0;i+c.length<=n&&0!=r.length;){c===t&&h++,l.push({position:i,length:c.length});var p=i+c.length;for(r.pop();0!=r.length&&(i=(s=r[r.length-1]).position,c=s.word,p>i);)r.pop()}return a+=h,{hits:l,start:o,end:n,searchTextCount:h}}var g=[];0!=u.length&&g.push(d(0,0,i.length,u));for(var v=[];0!=f.length;){var $=f[f.length-1],C=$.position,m=$.word,x=C-20,w=C+80;x<0&&(x=0),w<C+m.length&&(w=C+m.length),w>l.length&&(w=l.length),v.push(d(0,x,w,f))}v.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var y=parseInt("1");y>=0&&(v=v.slice(0,y));function T(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var b="";0!=g.length?b+="<li><a href='"+p+"' class='search-result-title'>"+T(i,g[0])+"</a>":b+="<li><a href='"+p+"' class='search-result-title'>"+i+"</a>",v.forEach(function(t){b+="<a href='"+p+'\'><p class="search-result">'+T(l,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:a,hitCount:s,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),!1===isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){27===t.which&&$(".search-popup").is(":visible")&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("2uqmIjYredIvFy6tEXbKG4Fj-gzGzoHsz","CWl1rE8cQlIseOg2Cq3hzxYi")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var i=0;i<e.length;i++){var s=e[i],r=s.get("url"),l=s.get("time"),c=document.getElementById(r);$(c).find(t).text(l)}for(i=0;i<n.length;i++){r=n[i],c=document.getElementById(r);var u=$(c).find(t);""==u.text()&&u.text(0)}}else o.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),s.setACL(r),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="default",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="hexo-helper-live2d"><canvas id="live2dcanvas" width="150" height="300"></canvas></div><style>#live2dcanvas{position:fixed;width:150px;height:300px;opacity:.7;right:-30px;z-index:999;pointer-events:none;bottom:40px}</style><script type="text/javascript" src="/live2d/device.min.js"></script><script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    var trElement = document.getElementById('hexo-helper-live2d');
    trElement.parentNode.removeChild(trElement);
    return;
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/hijiki.model.json", 0.5);});
})();
</script></body></html><script type="text/javascript" src="/js/src/love.js"></script>